package retail.dataframes

import com.typesafe.config.{Config, ConfigFactory}
import org.apache.hadoop.fs.FileSystem
import org.apache.log4j.{Level, Logger}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.{SQLContext, SparkSession}

object TotalRevenuePerDept {

  def main(args: Array[String]): Unit = {


    var CustomersPath = "hdfs://192.168.163.130:8020/user/cloudera/retailproject/customers"
    var CategoriesPath = "hdfs://192.168.163.130:8020/user/cloudera/retailproject/categories"
    var OrdersPath = "hdfs://192.168.163.130:8020/user/cloudera/retailproject/orders"
    var OrderItemsPath = "hdfs://192.168.163.130:8020/user/cloudera/retailproject/order_items"
    var DepartmentsPath = "hdfs://192.168.163.130:8020/user/cloudera/retailproject/departments"
    var ProductsPath = "hdfs://192.168.163.130:8020/user/cloudera/retailproject/products"

    var executionEnvironment = args(6)
    val props: Config = ConfigFactory.load()
    val conf = new SparkConf().
      setAppName("TotalRevenuePerDept").
      setMaster(props.getConfig(executionEnvironment).getString("executionMode"))
    val sc = SparkContext.getOrCreate(conf)
    val sqlContext = SQLContext.getOrCreate(sc)
    sqlContext.setConf("spark.sql.shuffle.partitions", "2")
    import sqlContext.implicits._


    System.setProperty("hadoop.home.dir", "C:\\Users\\Debu\\IdeaProjects\\bin\\");
    System.setProperty("spark.sql.warehouse.dir", "file:C:/Users/Debu/IdeaProjects/scala_practise/spark-warehouse");

    if (args.length < 1) {

      Logger.getLogger(TotalRevenuePerDept.toString).getLevel
      Logger.getLogger("org").setLevel(Level.OFF)
      Logger.getLogger("akka").setLevel(Level.OFF)


    }
    else {
      executionEnvironment = args(6)
      CustomersPath = args(0)
      CategoriesPath = args(1)
      OrdersPath = args(2)
      OrderItemsPath = args(3)
      DepartmentsPath = args(4)
      ProductsPath = args(5)
    }
    val CustomersPathDf = sqlContext.read
      .format("parquet")
      .parquet(CustomersPath)

    val CategoriesPathDf = sqlContext.read
      .format("parquet")
      .parquet(CategoriesPath)

    val OrdersPathDf = sqlContext.read
      .format("parquet")
      .parquet(OrdersPath)

    val OrderItemsPathDf = sqlContext.read
      .format("parquet")
      .parquet(OrderItemsPath)

    val DepartmentsPathDf = sqlContext.read
      .format("parquet")
      .parquet(DepartmentsPath)

    val ProductsPathDf = sqlContext.read
      .format("parquet")
      .parquet(ProductsPath)

    /**val spark = SparkSession.builder()
      .config("spark.sql.warehouse.dir", "file:///c:/tmp/spark-warehouse")
      .getOrCreate()**/

    //register dataframe as table to use in SQL way
    OrdersPathDf.registerTempTable("orders")
    CustomersPathDf.registerTempTable("customers")
    CategoriesPathDf.registerTempTable("categories")
    OrderItemsPathDf.registerTempTable("orderitems")
    DepartmentsPathDf.registerTempTable("departments")
    ProductsPathDf.registerTempTable("products")

    //Do the join and pick the needed columns
    val flattenDf = sqlContext.sql("""
                                     	  |SELECT
                                     	  |concat(customer_fname,customer_lname) as customer_name,
                                     	  |order_status ,
                                     	  |department_name,
                                     	  |SUM(order_item_subtotal) as Totalrevenue
                                     	 	|FROM
                                     	 	|	customers
                                     	 	|JOIN orders
                                     	 	|	ON customers.customer_id = orders.order_customer_id
                                     		|JOIN  order_items
                                     		|	ON orders.order_id =order_items.order_item_order_id
                                     		|JOIN  products
                                     		|	ON products.product_id = order_items.order_item_product_id
                                     		|JOIN  categories
                                     		|	ON categories.category_id = products.product_category_id
                                     		|JOIN in departments
                                     		|	ON departments.department_id = categories.category_department_id
                                   		""".stripMargin)
    //for testing in IDE
    flattenDf.show(10)
  }
}
